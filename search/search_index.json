{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introdu\u00e7\u00e3o ao Gerenciamento de Dados com Apache Iceberg e Delta Lake no PySpark","text":"<p>Este documento apresenta uma introdu\u00e7\u00e3o pr\u00e1tica ao uso das tecnologias Apache Iceberg e Delta Lake com PySpark para gerenciamento avan\u00e7ado de dados. Ambas as solu\u00e7\u00f5es s\u00e3o projetadas para aprimorar a manipula\u00e7\u00e3o de grandes volumes de dados em ambientes distribu\u00eddos, oferecendo suporte a transa\u00e7\u00f5es ACID, versionamento e opera\u00e7\u00f5es como inser\u00e7\u00e3o, atualiza\u00e7\u00e3o e exclus\u00e3o de registros.</p>"},{"location":"#apache-iceberg-com-pyspark","title":"Apache Iceberg com PySpark","text":"<p>Apache Iceberg \u00e9 um formato de tabela aberto para grandes datasets anal\u00edticos. Ele permite que voc\u00ea gerencie e consulte dados de forma eficiente e segura, garantindo a integridade dos dados mesmo em opera\u00e7\u00f5es concorrentes.</p> <p>No exemplo apresentado, configuramos uma sess\u00e3o Spark para trabalhar localmente com Iceberg, definindo um cat\u00e1logo Hadoop e criando uma tabela chamada <code>football_games</code> com v\u00e1rias colunas relacionadas a partidas de futebol. Al\u00e9m disso, mostramos como inserir novos dados, atualizar campos espec\u00edficos e deletar registros usando comandos SQL executados via Spark.</p>"},{"location":"#principais-pontos","title":"Principais pontos:","text":"<ul> <li>Configura\u00e7\u00e3o do SparkSession para suportar Iceberg.</li> <li>Cria\u00e7\u00e3o de tabela Iceberg com esquema definido.</li> <li>Inser\u00e7\u00e3o, atualiza\u00e7\u00e3o e dele\u00e7\u00e3o de dados utilizando Spark SQL.</li> </ul>"},{"location":"#delta-lake-com-pyspark","title":"Delta Lake com PySpark","text":"<p>Delta Lake \u00e9 uma camada de armazenamento open-source que traz funcionalidades de banco de dados transacional para data lakes. Ele garante confiabilidade, atomicidade e performance nas opera\u00e7\u00f5es sobre datasets grandes.</p> <p>Na demonstra\u00e7\u00e3o, iniciamos uma sess\u00e3o Spark configurada para Delta Lake, lemos dados CSV com esquema expl\u00edcito, salvamos no formato Delta e criamos uma tabela Delta. Tamb\u00e9m realizamos opera\u00e7\u00f5es t\u00edpicas como atualiza\u00e7\u00e3o e exclus\u00e3o de registros, al\u00e9m de inserir novos dados programaticamente.</p>"},{"location":"#principais-pontos_1","title":"Principais pontos:","text":"<ul> <li>Configura\u00e7\u00e3o do SparkSession para Delta Lake.</li> <li>Defini\u00e7\u00e3o de schema e leitura de arquivos CSV.</li> <li>Cria\u00e7\u00e3o de tabela Delta e opera\u00e7\u00f5es CRUD (Create, Read, Update, Delete).</li> <li>Uso da API Delta para manipula\u00e7\u00e3o eficiente dos dados.</li> </ul>"},{"location":"#consideracoes-finais","title":"Considera\u00e7\u00f5es Finais","text":"<p>Tanto Apache Iceberg quanto Delta Lake s\u00e3o solu\u00e7\u00f5es poderosas para gerenciar dados anal\u00edticos em larga escala. Elas oferecem uma base s\u00f3lida para construir pipelines de dados confi\u00e1veis, com suporte a transa\u00e7\u00f5es e controle de vers\u00f5es. A escolha entre elas depende do ambiente, necessidades espec\u00edficas e ecossistema adotado.</p> <p>No geral, aprender e aplicar essas tecnologias com PySpark facilita o desenvolvimento de sistemas de dados robustos, escal\u00e1veis e consistentes para an\u00e1lise e processamento.</p>"},{"location":"apache_iceberg/","title":"Documenta\u00e7\u00e3o: Uso do Apache Iceberg com PySpark para Tabela de Jogos de Futebol","text":""},{"location":"apache_iceberg/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>Este projeto demonstra como configurar um ambiente local usando Apache Iceberg junto com PySpark para manipular uma tabela que armazena dados de partidas de futebol. A tabela \u00e9 criada, dados s\u00e3o inseridos, atualizados e deletados usando comandos SQL no Spark.</p>"},{"location":"apache_iceberg/#configuracao-do-sparksession-com-iceberg","title":"Configura\u00e7\u00e3o do SparkSession com Iceberg","text":"<pre><code>from pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n  .appName(\"IcebergLocalDevelopment\") \\\n  .config('spark.jars.packages', 'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.6.1') \\\n  .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n  .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n  .config(\"spark.sql.catalog.local.type\", \"hadoop\") \\\n  .config(\"spark.sql.catalog.local.warehouse\", \"spark-warehouse/iceberg\") \\\n  .getOrCreate()\n</code></pre>"},{"location":"apache_iceberg/#explicacao","title":"Explica\u00e7\u00e3o","text":"<p>\u00c9 A conex\u00e3o do pyspark com o delta lake</p>"},{"location":"apache_iceberg/#criacao-da-tabela-iceberg","title":"Cria\u00e7\u00e3o da Tabela Iceberg","text":"<pre><code>CREATE TABLE IF NOT EXISTS local.db.football_games (\n  Round STRING,\n  Date STRING,\n  Time STRING,\n  Team STRING,\n  Team_Score STRING,\n  Opponent_Score STRING,\n  Opponent STRING,\n  Home_Score_AET STRING,\n  Away_Score_AET STRING,\n  Home_Penalties STRING,\n  Away_Penalties STRING,\n  Team_Points STRING,\n  Opponent_Points STRING,\n  season STRING,\n  Location STRING,\n  Country STRING,\n  Competition STRING\n)\nUSING iceberg\nLOCATION 'spark-warehouse/iceberg/db/football_games'\n</code></pre>"},{"location":"apache_iceberg/#explicacao_1","title":"Explica\u00e7\u00e3o","text":"<ul> <li>Cria a tabela football_games dentro do cat\u00e1logo local, banco db.</li> <li>Define as colunas e seus tipos (todos STRING no exemplo, pode ser ajustado conforme necessidade).</li> <li>Especifica USING iceberg para usar o formato Iceberg.</li> <li>Usa a localiza\u00e7\u00e3o f\u00edsica no disco para armazenar os dados.</li> </ul>"},{"location":"apache_iceberg/#inserindo-dados-na-tabela","title":"Inserindo Dados na Tabela","text":"<pre><code>spark.sql(\"\"\"\n    INSERT INTO local.db.football_games VALUES (\n        'ROUND 2',          -- Round\n        '15/09/2002',       -- Date\n        '20:00',            -- Time\n        'NEW TEAM',         -- Team\n        3.0,                -- Team_Score\n        1.0,                -- Opponent_Score\n        'OLD TEAM',         -- Opponent\n        NULL,               -- Home_Score_AET\n        NULL,               -- Away_Score_AET\n        NULL,               -- Home_Penalties\n        NULL,               -- Away_Penalties\n        3.0,                -- Team_Points\n        0.0,                -- Opponent_Points\n        2002,               -- season\n        'Home',             -- Location\n        'spain',            -- Country\n        'primera-division'  -- Competition\n    )\n\"\"\")\n</code></pre>"},{"location":"apache_iceberg/#explicacao_2","title":"Explica\u00e7\u00e3o","text":"<ul> <li>Insere uma nova linha na tabela.</li> <li>Os valores devem estar na ordem e no tipo correspondente das colunas.</li> </ul>"},{"location":"apache_iceberg/#atualizando-dados","title":"Atualizando Dados","text":"<pre><code>spark.sql(\"\"\"\n    UPDATE local.db.football_games\n    SET Team_Score = '1.0'\n    WHERE Team = 'RACING SANTANDER' AND Date = '31/08/2002'\n\"\"\")\n</code></pre>"},{"location":"apache_iceberg/#explicacao_3","title":"Explica\u00e7\u00e3o","text":"<p>-Atualiza o campo Team_Score para '1.0' para partidas espec\u00edficas.</p>"},{"location":"apache_iceberg/#deletando-dados","title":"Deletando Dados","text":"<pre><code>spark.sql(\"\"\"\n    DELETE FROM local.db.football_games\n    WHERE Team = 'RAYO VALLECANO' AND Date = '01/09/2002'\n\"\"\")\n</code></pre>"},{"location":"apache_iceberg/#explicacao_4","title":"Explica\u00e7\u00e3o","text":"<ul> <li>Remove linhas da tabela que atendem \u00e0 condi\u00e7\u00e3o especificada.</li> </ul>"},{"location":"apache_iceberg/#consideracoes-finais","title":"Considera\u00e7\u00f5es Finais","text":"<ul> <li>Apache Iceberg oferece suporte ACID, versionamento e consultas eficientes em grandes volumes de dados.</li> <li>Usar o SparkSQL facilita manipula\u00e7\u00e3o declarativa das tabelas.</li> <li>A configura\u00e7\u00e3o local \u00e9 ideal para desenvolvimento e testes, para produ\u00e7\u00e3o use um cat\u00e1logo adequado (Hive, Glue, etc.).</li> </ul>"},{"location":"delta_lake/","title":"Documenta\u00e7\u00e3o do C\u00f3digo Delta Lake com PySpark","text":"<p>Este documento explica o c\u00f3digo fornecido para manipula\u00e7\u00e3o de dados usando PySpark e Delta Lake.</p>"},{"location":"delta_lake/#inicializacao-do-sparksession-com-delta-lake","title":"Inicializa\u00e7\u00e3o do SparkSession com Delta Lake","text":"<pre><code>from pyspark.sql import SparkSession\n\nspark = (\n    SparkSession\n    .builder\n    .master(\"local[*]\")\n    .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.12:3.2.0\")\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n    .getOrCreate()\n)\n</code></pre>"},{"location":"delta_lake/#explicacao","title":"Explica\u00e7\u00e3o","text":"<ul> <li>Cria uma sess\u00e3o Spark configurada para usar o Delta Lake.</li> <li>Configura os pacotes e extens\u00f5es necess\u00e1rios para Delta Lake.</li> </ul>"},{"location":"delta_lake/#definicao-do-schema-para-o-csv","title":"Defini\u00e7\u00e3o do Schema para o CSV","text":"<pre><code>from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n\nschema = StructType([\n    StructField(\"id\", IntegerType(), True),\n    StructField(\"match_url\", StringType(), True),\n    StructField(\"match_id\", IntegerType(), True),\n    StructField(\"team_A\", StringType(), True),\n    StructField(\"team_B\", StringType(), True),\n    StructField(\"score_tA\", IntegerType(), True),\n    StructField(\"score_tB\", IntegerType(), True),\n    StructField(\"competition\", StringType(), True),\n    StructField(\"type_of_match\", StringType(), True)\n])\n</code></pre>"},{"location":"delta_lake/#explicacao_1","title":"Explica\u00e7\u00e3o","text":"<ul> <li>Define a estrutura dos dados esperados no CSV.</li> <li>Garante que os dados sejam lidos com os tipos corretos.</li> </ul>"},{"location":"delta_lake/#leitura-do-csv-com-schema-aplicado","title":"Leitura do CSV com Schema Aplicado","text":"<pre><code>csv_path = '../data/HLTVCsGoResults.csv'\n\ndf = spark.read.option(\"header\", True).option(\"sep\", \";\").schema(schema).csv(csv_path)\n\n</code></pre>"},{"location":"delta_lake/#explicacao_2","title":"Explica\u00e7\u00e3o","text":"<ul> <li>L\u00ea o arquivo CSV usando o esquema definido.</li> <li>Especifica que o separador \u00e9 ; e que o arquivo tem cabe\u00e7alho.</li> </ul>"},{"location":"delta_lake/#escrita-dos-dados-no-delta-lake","title":"Escrita dos Dados no Delta Lake","text":"<pre><code>delta_path = './spark-warehouse/csgo_matches'\n\ndf.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(delta_path)\n</code></pre>"},{"location":"delta_lake/#explique","title":"Explique","text":"<ul> <li>Salva o DataFrame no formato Delta no caminho especificado.</li> <li>Usa modo overwrite para substituir dados antigos.</li> <li>Permite mesclar esquemas com mergeSchema.</li> </ul>"},{"location":"delta_lake/#criacao-da-tabela-delta-no-spark-sql","title":"Cria\u00e7\u00e3o da Tabela Delta no Spark SQL","text":"<pre><code>spark.sql(f\"\"\"\nCREATE TABLE IF NOT EXISTS csgo_matches\nUSING DELTA\nLOCATION '{delta_path}'\n\"\"\")\n</code></pre>"},{"location":"delta_lake/#explicacao_3","title":"Explica\u00e7\u00e3o","text":"<ul> <li>Cria uma tabela SQL que aponta para os arquivos Delta.</li> </ul>"},{"location":"delta_lake/#atualizacao-de-dados-na-tabela-delta","title":"Atualiza\u00e7\u00e3o de Dados na Tabela Delta","text":"<pre><code>from delta.tables import DeltaTable\n\ndelta_table = DeltaTable.forPath(spark, delta_path)\n\ndelta_table.update(\n    condition=\"match_id = 2321666\",\n    set={\"score_tB\": \"2\"}\n)\n</code></pre>"},{"location":"delta_lake/#explicacao_4","title":"Explica\u00e7\u00e3o","text":"<ul> <li>Atualiza o campo score_tB onde match_id \u00e9 2321666.</li> </ul>"},{"location":"delta_lake/#delecao-de-dados-na-tabela-delta","title":"Dele\u00e7\u00e3o de Dados na Tabela Delta","text":"<pre><code>delta_table.delete(\"match_id = 2321666\")\n</code></pre>"},{"location":"delta_lake/#explicacao_5","title":"Explica\u00e7\u00e3o","text":"<ul> <li>Remove as linhas que t\u00eam match_id igual a 2321666.</li> </ul>"},{"location":"delta_lake/#insercao-de-novos-dados","title":"Inser\u00e7\u00e3o de Novos Dados","text":"<pre><code>from pyspark.sql import Row\nfrom pyspark.sql.functions import col\n\nnew_row = [Row(\n    match_url=\"https://www.hltv.org/matches/9999999/test-vs-test\",\n    match_id=9999999,\n    team_A=\"TeamTestA\",\n    team_B=\"TeamTestB\",\n    score_tA=16,\n    score_tB=14,\n    competition=\"Test Cup\",\n    type_of_match=\"bo3\"\n)]\n\nnew_data = spark.createDataFrame(new_row)\n\nnew_data = new_data.select(\n    col(\"match_url\").cast(\"string\"),\n    col(\"match_id\").cast(\"int\"),\n    col(\"team_A\").cast(\"string\"),\n    col(\"team_B\").cast(\"string\"),\n    col(\"score_tA\").cast(\"int\"),\n    col(\"score_tB\").cast(\"int\"),\n    col(\"competition\").cast(\"string\"),\n    col(\"type_of_match\").cast(\"string\")\n)\n\nnew_data.write.format(\"delta\").mode(\"append\").save(delta_path)\n</code></pre>"},{"location":"delta_lake/#explicacao_6","title":"Explica\u00e7\u00e3o","text":"<ul> <li>Cria um novo registro e transforma em DataFrame.</li> <li>Ajusta os tipos das colunas para combinar com o schema da tabela.</li> <li>Insere o novo dado na tabela Delta no modo append.</li> </ul>"},{"location":"delta_lake/#conclusao","title":"Conclus\u00e3o","text":"<p>Neste exemplo, vimos como utilizar o Delta Lake integrado ao PySpark para gerenciar dados de forma eficiente, combinando funcionalidades de transa\u00e7\u00f5es ACID, versionamento e performance. O Delta Lake simplifica opera\u00e7\u00f5es complexas como atualiza\u00e7\u00e3o, dele\u00e7\u00e3o e inser\u00e7\u00e3o de dados em grandes volumes, mantendo a consist\u00eancia e integridade das informa\u00e7\u00f5es no data lake. Essa abordagem \u00e9 muito \u00fatil para pipelines de dados confi\u00e1veis e escal\u00e1veis.</p>"}]}